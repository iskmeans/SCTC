\documentclass{article}

\usepackage[colorlinks=true]{hyperref}
\usepackage[cmex10]{amsmath}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}

\newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\left(#1\right)}}
\DeclareMathOperator*{\argmin}{\mathrm{argmin}}

\begin{document}

\title{SubKit: Subspace Clustering Library}
\author{Stephen Tierney, Yi Guo and Junbin Gao}
\maketitle

\tableofcontents

\newpage
\section{Motivation}

SubKit is a library with implementations for subspace clustering learning algorithms. This library was created during the writing of the OSC journal article to ensure fair comparison between different methods. In the original OSC CVPR paper we used implementations provided by authors of SSC and LRR. However these implementations used different noise models to what we suggested with OSC. For example the original LRR implementation expects column wise noise while we expect Gaussian noise in all columns. Therefore we required new implementations with the same noise model for fair comparison in our journal article.

\section{Preliminaries}

We introduce required notation. We assume the following noise model
\begin{align}
\mathbf{X = A + N}
\end{align}
where $\mathbf A$ is the noise free data (column wise data samples), $\mathbf N$ is some noise and $\mathbf X$ is the observed data. We assume that each data sample in $\mathbf A$ lies exactly on its corresponding subspace. In an ideal case subspace clustering uses the self expressive model with the noise free data i.e.\
\begin{align}
\mathbf{A = AZ}
\end{align}
where $\mathbf Z$ is the matrix of coefficients. However this is rarely the case since we do not observe noise free data therefore the self expressive model becomes 
\begin{align}
\mathbf{X = XZ + E}
\end{align}
where $\mathbf E$ is a fitting error. Once $\mathbf Z$ has been estimated one performs spectral clustering to obtain the final segmentation.

\newpage
\section{Function Listing}

\begin{table}[!h]
{\small{
\centering

\begin{tabular}{c | c | c}
\hline
Objective & Function & Section \\
\hline

$\begin{array}{c} \min_{\mathbf Z} \; \|\mathbf Z\|_{1} \\
\text{s.t.} \; \mathbf{A = AZ} \end{array}$
	& ssc\_noisefree	& 4.1  \\
\hline

\multirow{4}{*}{$\begin{array}{c} \min_{\mathbf Z} \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \lambda\|\mathbf Z\|_{1} \\
\text{s.t.} \quad \mathbf{X = XZ + E}\text{, diag}(\mathbf Z) = \mathbf 0 \end{array}$}
		& ssc\_relaxed	& 4.2 \\
		& ssc\_relaxed\_lin	& 4.3.1 \\
		& ssc\_relaxed\_lin\_ext	& 4.3.2 \\
		& ssc\_relaxed\_lin\_acc	& 4.3.3 \\		
\hline

$\begin{array}{c} \min_{\mathbf{Z, E}}  \frac12\|\mathbf E\|^2_F+  \lambda\|\mathbf Z\|_{1} \\
\text{s.t.} \quad \mathbf{X = XZ + E}\text{, diag}(\mathbf Z) = \mathbf 0 \end{array}$
	& ssc\_exact\_fro	& 4.4  \\
\hline

$\begin{array}{c} \min_{\mathbf{Z, E}}  \|\mathbf E\|_1+  \lambda\|\mathbf Z\|_{1} \\
\text{s.t.} \quad \mathbf{X = XZ + E}\text{, diag}(\mathbf Z) = \mathbf 0\end{array}$
	& ssc\_exact\_l1	& 4.4  \\
\hline

$\begin{array}{c} \min_{\mathbf{Z, E}}  \|\mathbf E\|_{1,2}+  \lambda\|\mathbf Z\|_{1} \\
\text{s.t.} \quad \mathbf{X = XZ + E}\text{, diag}(\mathbf Z) = \mathbf 0 \end{array}$
	& ssc\_exact\_l1l2	& 4.4  \\
\hline

$\begin{array}{c} \min_{\mathbf Z} \; \|\mathbf Z\|_{*} \\
\text{s.t.} \; \mathbf{A = AZ} \end{array}$
	& lrr\_noisefree	& 5.1  \\
\hline

\multirow{4}{*}{$\begin{array}{c} \min_{\mathbf Z} \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \lambda\|\mathbf Z\|_{*} \end{array}$}
		& lrr\_relaxed	& 5.2 \\
		& lrr\_relaxed\_lin	& 5.3 \\
		& lrr\_relaxed\_lin\_ext	& 5.3 \\
		& lrr\_relaxed\_lin\_acc	& 5.3 \\		
\hline

$\begin{array}{c} \min_{\mathbf{Z, E}}  \frac12\|\mathbf E\|^2_F+  \lambda\|\mathbf Z\|_{*} \\
\text{s.t.} \quad \mathbf{X = XZ + E} \end{array}$
	& lrr\_exact\_fro	& 5.4  \\
\hline

$\begin{array}{c} \min_{\mathbf{Z, E}}  \|\mathbf E\|_1+  \lambda\|\mathbf Z\|_{*} \\
\text{s.t.} \quad \mathbf{X = XZ + E} \end{array}$
	& lrr\_exact\_l1	& 5.4  \\
\hline

$\begin{array}{c} \min_{\mathbf{Z, E}}  \|\mathbf E\|_{1,2}+  \lambda\|\mathbf Z\|_{*} \\
\text{s.t.} \quad \mathbf{X = XZ + E} \end{array}$
	& lrr\_exact\_l1l2	& 5.4  \\
\hline

\multirow{3}{*}{$\begin{array}{c} \min_{\mathbf {Z, A, N}} \; \lambda \|\mathbf{N}\|_{q} + \|\mathbf Z\|_{*} \\
\text{s.t.} \quad \mathbf{A = AZ, X = A + N} \end{array}$}
	& r\_lrr\_fro	& 5.5  \\
	& r\_lrr\_l1		& 5.5  \\
	& r\_lrr\_l2		& 5.5  \\	
\hline

$\begin{array}{c} \min_{\mathbf Z} \frac12\|\mathbf{X - X Z}|^2_F +\lambda_1\|\mathbf Z\|_{1}+\lambda_2\|\mathbf Z\mathbf R\|_{1} \\
\text{s.t.} \; \text{diag}(\mathbf Z) = \mathbf 0 \end{array}$
	& spatsc\_noisefree	& 6.1  \\
\hline

$\begin{array}{c} \min_{\mathbf Z} \frac12\|\mathbf{X - X Z}|^2_F +\lambda_1\|\mathbf Z\|_{1}+\lambda_2\|\mathbf Z\mathbf R\|_{1,2} \\
\text{s.t.} \; \text{diag}(\mathbf Z) = \mathbf 0 \end{array}$
	& osc\_noisefree	& 7.1  \\
\hline

\end{tabular}
}}
\end{table}

\newpage
\section{Sparse Subspace Clustering}

\subsection{Noise Free}

\begin{align}
\min_{\mathbf Z} \lambda\|\mathbf Z\|_{1} \\
\text{s.t.} \quad \mathbf{A = AZ} \nonumber
\end{align}
The solution to the above objective has a fast approximate solution, given by the Shape Interaction Matrix. See the low-rank clustering section for further details. Alternatively one can calculate the sample correlation matrix $\mathbf X^T \mathbf X$ and select points with the maximum correlation as neighbours on the graph.

\subsection{Relaxed SSC ADMM}

\begin{align}
\min_{\mathbf Z} \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \lambda\|\mathbf Z\|_{1}
\end{align}
To solve the relaxation via ADMM one must incorporate an auxiliary variable
\begin{align}
\min_{\mathbf{Z, J}} \frac12\|\mathbf X - \mathbf X\mathbf J\|^2_F + \lambda\|\mathbf Z\|_{1} \\
\text{s.t.} \quad \mathbf{Z = J} \nonumber
\end{align}
\begin{align}
\min_{\mathbf{Z, J}} \frac12\|\mathbf X - \mathbf X\mathbf J\|^2_F + \lambda\|\mathbf Z\|_{1} + \langle \mathbf{Y, Z - J} \rangle + \frac{\mu}{2} \| \mathbf{Z - J} \|_F^2
\end{align}
Then iterate the following
\begin{enumerate}
\item Fix others and solve for $\mathbf J$
\[
\min_{\mathbf J} \frac12\|\mathbf X - \mathbf X\mathbf J\|^2_F - \langle \mathbf{Y, J} \rangle + \frac{\mu}{2} \| \mathbf{Z - J} \|_F^2
\]
\[
\min_{\mathbf J} \frac12\|\mathbf X - \mathbf X\mathbf J\|^2_F + \frac{\mu}{2} \| (\mathbf Z + \frac{1}{\mu} \mathbf Y ) - \mathbf J \|_F^2
\]
\[
\mathbf X^T(\mathbf X - \mathbf X\mathbf J) + \mu ( (\mathbf Z + \frac{1}{\mu} \mathbf Y ) - \mathbf J )
\]
\[
\mathbf X^T\mathbf X\mathbf J  + \mu \mathbf J = \mathbf X^T\mathbf X + \mu \mathbf Z + \mathbf Y
\]
\[
(\mathbf X^T\mathbf X + \mu \mathbf I )\mathbf J = \mathbf X^T\mathbf X + \mu \mathbf Z + \mathbf Y
\]
\[
\mathbf J = (\mathbf X^T\mathbf X + \mu \mathbf I )^{-1} (\mathbf X^T\mathbf X + \mu \mathbf Z + \mathbf Y)
\]

\item Fix others and solve for $\mathbf Z$
\[
\min_{\mathbf Z} \lambda\|\mathbf Z\|_{1} + \langle \mathbf{Y, Z} \rangle + \frac{\mu}{2} \| \mathbf{Z - J} \|_F^2
\]
\[
\min_{\mathbf Z} \lambda\|\mathbf Z\|_{1} + \frac{\mu}{2} \| \mathbf Z - (\mathbf J - \frac{1}{\mu} \mathbf Y) \|_F^2
\]

\item Update $\mathbf Y$
\[
\mathbf Y = \mathbf Y + \mu (\mathbf{Z - J})
\]

\item Check stopping criteria
\[
\| \mathbf Z - \mathbf J \|_F  < \epsilon_1, \;
\mu \; \textrm{max} ( \| \mathbf Z_{k+1} - \mathbf Z_{k}  \|_F  , \|  \mathbf J_{k+1} - \mathbf J_{k} \|_F) < \epsilon_2
\]

\end{enumerate}

\subsection{Relaxed SSC Linearised}

Instead of using ADMM one can linearise the Frobenius norm term instead. This removes the need to do an expensive matrix inversion.
\begin{align}
\min_{\mathbf Z} L = \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \lambda\|\mathbf Z\|_{1}
\end{align}

Let $F = \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F$ and $\partial F = - \mathbf X^T \mathbf X + \mathbf X^T \mathbf{X Z}$.

\subsubsection{Gradient Descent}

This version consists on iterating the following until convergence

\[
\min_{\mathbf Z} \widetilde{L}_{\rho}(\mathbf{Z, Z_k}) =  \lambda\|\mathbf Z\|_{1} + \frac{\rho}{2} \| \mathbf Z - ( \mathbf Z_{k-1} - \frac{1}{\rho} \partial F(\mathbf Z_{k-1})) \|_F^2
\]

\subsubsection{Extended Gradient Algorithm}

It has been shown that through the correct choice of step size $\rho$ the gradient method can achieve a convergence rate of $\BigO{\frac{1}{k}}$ \cite{ji2009accelerated}. More specifically we require that $\rho \geq C$ where $C$ is the Lipschitz constant. However we do not know the value of $C$ in advance. Fortunately we know that if $L(\mathbf B) \leq \widetilde{L}_{\rho}(\mathbf B, \mathbf Z_{k-1})$ where $\mathbf B = \mathcal S_{\frac{\lambda}{\rho}}(\mathbf Z_{k-1} - \frac1{\lambda}\partial F(\mathbf Z_{k-1}))$ then $\rho \geq C$. 

\begin{algorithm}
\caption{Extended Gradient Descent for Robust MC}
\begin{algorithmic}

\REQUIRE $k = 1$, $r_0 = \infty$, $\mathbf Z_0 = \mathbf 0$, $\lambda$, $\rho$, $\gamma$, $\epsilon$

\WHILE{$r_k - r_{k-1} \geq \epsilon$ }

	\WHILE{$L(\mathbf B) \geq \widetilde{L}_{\rho}(\mathbf B, \mathbf Z_{k-1})$}
	
		\STATE $\rho = \gamma \rho$	
	
	\ENDWHILE

	\STATE $\mathbf Z_k = \mathcal S_{\frac{\lambda}{\rho}}(\mathbf Z_{k-1} - \frac1{\rho}\partial F(\mathbf Z_{k-1}))$
	\STATE $r_k = \frac12\|\mathbf X - \mathbf X\mathbf Z_k\|^2_F + \lambda\|\mathbf Z_k\|_{1}$
	\STATE $k = k + 1$
	
\ENDWHILE

\end{algorithmic}
\end{algorithm}

\subsubsection{Accelerated Gradient Algorithm}

We can further improve our convergence rate to $\BigO{\frac{1}{k^2}}$ by adopting Nesterov's accelerated gradient algorithm. This is similar to the algorithm described in \cite{ji2009accelerated}. Let $\mathbf B = \mathcal S_{\frac{\lambda}{\rho}}(\mathbf J_{k-1} - \frac1{\rho}\partial F(\mathbf J_{k-1}))$.

\begin{algorithm}
\caption{Accelerated Gradient Descent for Robust MC}
\begin{algorithmic}

\REQUIRE $k = 1$, $r_0 = \infty$, $\mathbf Z_0 = \mathbf 0$,  $\mathbf J_0 = \mathbf 0$, $\alpha_0 = 1$, $\lambda$, $\rho$, $\gamma$, $\epsilon$

\WHILE{$r_k - r_{k-1} \geq \epsilon$ }

	\WHILE{$L(\mathbf B) \geq \widetilde{L}_{\rho}(\mathbf B, \mathbf J_{k-1})$}
	
		\STATE $\rho = \gamma \rho$	
	
	\ENDWHILE

	\STATE $\mathbf Z_k = \mathcal S_{\frac{\lambda}{\rho}}(\mathbf J_{k-1} - \frac1{\rho}\partial F(\mathbf J_{k-1}))$
	\STATE $\alpha_{k} = \frac{1 + \sqrt{1 + 4 \alpha_{k-1}^2)}}{2}$
	\STATE $\mathbf J_{k} = \mathbf Z_k + \left ( \frac{\alpha_{k-1} - 1}{\alpha_{k}} \right ) (\mathbf Z_k - \mathbf Z_{k-1})$
	\STATE $r_k = \frac12\|\mathbf X - \mathbf X\mathbf Z_k\|^2_F + \lambda\|\mathbf Z_k\|_{1}$
	\STATE $k = k + 1$
	
\ENDWHILE

\end{algorithmic}
\end{algorithm}


\subsection{Exact SSC LADM}

Using LADM and exact constraints allows us to modify the noise term from Gaussian noise only to any other norm. The objective becomes
\begin{align}
\min_{\mathbf{E, Z}} \frac12\|\mathbf E \|^2_q + \lambda\|\mathbf Z\|_{1} \\
\text{s.t.} \quad \mathbf{X = XZ + E} \nonumber
\end{align}
where $q$ is a placeholder for norms such as $\ell_1, F, \ell_{1,2}$ etc.
\[
\min_{\mathbf{E, Z}} \frac12\|\mathbf E \|^2_q + \lambda\|\mathbf Z\|_{1} + \langle \mathbf{Y, XZ - X + E } \rangle + \frac{\mu}{2} \|\mathbf{XZ - X + E } \|_F^2 
\]

Iterate the following
\begin{enumerate}
\item Fix others  solve for $\mathbf Z$
\[
\min_{\mathbf{Z}} \lambda\|\mathbf Z\|_{1} + \langle \mathbf{Y, XZ} \rangle + \frac{\mu}{2} \|\mathbf{XZ - (X - E) } \|_F^2 
\]
\[
\min_{\mathbf{Z}} \lambda\|\mathbf Z\|_{1} + \frac{\mu}{2} \|\mathbf{XZ - (X - E} - \frac{1}{\mu} \mathbf Y ) \|_F^2 
\]
Let $F = \frac{\mu}{2} \|\mathbf{XZ - (X - E} - \frac{1}{\mu} \mathbf Y ) \|_F^2$ and $\partial F =  \mu \mathbf X^T (\mathbf{XZ - (X - E} - \frac{1}{\mu} \mathbf Y ))$
\[
\min_{\mathbf{Z}} \lambda\|\mathbf Z\|_{1} + \frac{\rho}{2} \| \mathbf Z - (\mathbf Z_{k} - \frac{1}{\rho} \partial F(\mathbf Z_{k}) \|_F^2 
\]


\item Fix others and solve for $\mathbf E$
\[
\min_{\mathbf{E}} \frac12\|\mathbf E \|^2_q + \langle \mathbf{Y, E} \rangle + \frac{\mu}{2} \|\mathbf{XZ - X + E } \|_F^2 
\]
\[
\min_{\mathbf{E}} \frac12\|\mathbf E \|^2_q + \langle \mathbf{Y, E} \rangle + \frac{\mu}{2} \|\mathbf{E - (X - XZ)} \|_F^2 
\]
\[
\min_{\mathbf{E}} \frac12\|\mathbf E \|^2_q  + \frac{\mu}{2} \|\mathbf{E - (X - XZ} - \frac{1}{\mu} \mathbf Y) \|_F^2 
\]


\item Update $\mathbf Y$
\begin{align*}
\mathbf Y =& \mathbf Y + \mu (\mathbf{XZ - X + E})
\end{align*}

\item Update $\mu$
\begin{align*}
 \mu = \textrm{min}( \mu_{\text{max}}, \gamma \mu)
\end{align*}
where $\gamma$ is defined as
\[
\gamma = 
\begin{cases}
\gamma_0 & \text{if} \;\; \mu_k \sqrt{\rho} \frac{\textrm{max} (   \| \mathbf Z_{k+1} - \mathbf Z_{k}  \|_F  , \|  \mathbf E_{k+1} - \mathbf E_{k} \|_F)}{\| \mathbf X \|_F} < \epsilon \\
1 & \text{otherwise,}
\end{cases}
\]
and $\rho > \| \mathbf X \|_F^2$,  $\mu_{\text{max}} >>  \mu_0$ and $\epsilon > 0$.

\item Check stopping criteria
\[
\frac{\| \mathbf X \mathbf Z_{k+1} - \mathbf X + \mathbf E_{k+1}  \|_F}{ \| \mathbf X \|_F} < \epsilon_1, \;
 \mu_k \sqrt{\rho} \frac{\textrm{max} ( \| \mathbf Z_{k+1} - \mathbf Z_{k}  \|_F  , \|  \mathbf E_{k+1} - \mathbf E_{k} \|_F)}{\| \mathbf X \|_F} < \epsilon_2
\]

\end{enumerate}

\subsection{Diagonal Constraint}

In some cases it may be desirable to enforce the constraint $\textrm{diag}(\mathbf Z) = \mathbf 0$ i.e.\ we should not allow each data point to be represented by itself. To enforce such a constraint it is not necessary to significantly alter the aforementioned optimisation schemes. This constraint only affects the step involving $\mathbf Z$. Since this step is the soft shrinkage operator and is separable at the element level one can simply set the diagonal entries to $0$ afterwards.

\newpage
\section{Low-Rank Subspace Clustering}

\subsection{Noise Free}

\begin{align}
\min_{\mathbf{Z}} \;  \| \mathbf{Z} \|_* \\
\text{s.t.} \quad \mathbf{A = AZ} \nonumber
\end{align}

The solution to the above objective is given by the Shape Interaction Matrix (SIM) which is defined as $\mathbf{V  V^T}$ where $\mathbf V$ is the right singular vectors of $\mathbf A$ i.e.\
\[
\mathbf{A = U \Sigma V^T}, \;\; \mathbf \Sigma = \text{diag}(\{\sigma_i\}_{i=1}^r)
\]

\subsection{Relaxed LRR ADMM}

\begin{align}
\min_{\mathbf Z} \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \tau \|\mathbf Z\|_{*}
\end{align}
This is solved similarly to Relaxed SSC ADMM. Instead of the $\ell_1$ shrinking operator in the update step for $\mathbf Z$ we must use the singular value shrinking operator which is defined as 
\begin{align}
\mathcal D_{\tau}(\mathbf Y) = \mathbf U S_{\tau}(\mathbf \Sigma) \mathbf V^T, \;\; S_{\tau}(\mathbf \Sigma) = \text{diag}(\{\text{max}(\sigma_i - \tau, 0)\}).
\end{align}

\subsection{Relaxed LRR Linearised}

This is solved similarly to Relaxed SSC ADMM, again replacing the $\ell_1$ with the singular value shrinking operator.

\subsection{Exact LRR LADM}

This is solved similarly to Exact SSC LADM, again replacing the $\ell_1$ with the singular value shrinking operator.

\subsection{Robust LRR}

An extension of LRR called Robust-LRR (R-LRR) aims to directly deal with noisy data using the previously described data generation model
\begin{align}
\min_{\mathbf {Z, A, N}} \; \lambda \|\mathbf{N}\|_{q} + \|\mathbf Z\|_{*} \\
\text{s.t.} \quad \mathbf{A = AZ, X = A + N} \nonumber
\end{align}
This objective can be decomposed into two steps. The first step solves a variant of RPCA [CITE] and the second uses the estimated $\mathbf A$ to compute $\mathbf Z$ using the SIM. However it is unclear as to whether such a procedure will be capable of exactly removing all noise, thus the SIM may give poor results. Fortunately further work in nuclear norm based subspace clustering by Vidal and Favaro \cite{Vidal201447} demonstrated a number of other closed form solutions for noisy data.

\newpage
\section{Spatial Subspace Clustering}

In \cite{Guo.Y;Gao.J;Li.F-2013} the following spatial subspace clustering (SpatSC) objective was proposed
\begin{align}
\label{YiGuo1}
\min_{\mathbf Z, \mathbf E} \frac12\|\mathbf E|^2_F +\lambda_1\|\mathbf Z\|_{1}+\lambda_2\|\mathbf Z\mathbf R\|_{1} \\
\text{s.t.} \quad \mathbf{X = XZ + E}\text{, diag}(\mathbf Z) = \mathbf 0 \nonumber
\end{align}

\subsection{Implementation}

SpatSC can be implemented similarly to OSC. However the $\ell_{1,2}$ shrinkage operator is replaced with $\ell_1$ shrinkage operator. Please see the following section for details.

\newpage
\section{Ordered Subspace Clustering}

\begin{align}
\label{objective}
\min_{\mathbf Z, \mathbf E} \frac12\|\mathbf E \|^2_F +\lambda_1\|\mathbf Z\|_{1}+\lambda_2\|\mathbf Z\mathbf R\|_{1,2} \\
\text{s.t.} \quad \mathbf{X = XZ + E} \nonumber
\end{align}

\subsection{Relaxed Constraints}

First we remove the variable $\mathbf E$ by using the constraint and thus the objective \eqref{objective} can be re-written as follows,
\begin{align}
\label{objective_relaxed}
\min_{\mathbf Z} \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F +\lambda_1\|\mathbf Z\|_{1}+\lambda_2\|\mathbf J \|_{1,2}\\
\text{s.t.} \quad \mathbf{J = ZR} \nonumber
\end{align}

Then the Augmented Lagrangian for the two introduced constraints is
\begin{align}
\mathcal{L}(\mathbf Z, \mathbf J) = & \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \lambda_1 \|\mathbf Z\|_{1} + \lambda_2\|\mathbf J\|_{1,2} \notag\\
& + \langle \mathbf Y_1, \mathbf J - \mathbf Z\mathbf R\rangle +\frac{\mu^k}2\|\mathbf J - \mathbf Z\mathbf R\|^2_F 
\label{objectiveADMM}
\end{align}

We can solve \eqref{objectiveADMM} for $\mathbf Z$ and $\mathbf J$ in an alternative manner when fixing the others, respectively.

\begin{enumerate}

\item Set $\mathbf J = \mathbf J^k$, solve for $\mathbf Z^{k+1}$ by
\begin{align*}
\lambda_1 \|\mathbf Z\|_{1} + \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \langle \mathbf Y, \mathbf J - \mathbf Z\mathbf R\rangle +\frac{\mu^k}2\|\mathbf J - \mathbf Z\mathbf R\|^2_F 
\end{align*}
which is equivalent to
\begin{align*}
\lambda_1 \|\mathbf Z\|_{1} + \frac12\|\mathbf X - \mathbf X\mathbf Z\|^2_F + \frac{\mu^k}2\|\mathbf J - \mathbf Z\mathbf R + \frac{1}{\mu} \mathbf Y \|^2_F
\end{align*}
We linearise the last two terms which we call $F$
\begin{align*}
\min_{\mathbf Z} \lambda_1 \| \mathbf Z \|_1 + \frac{\rho}{2} \| \mathbf Z - (\mathbf Z_{k-1} - \frac{1}{\rho} \partial F) \|_F^2
\end{align*}
and $\partial F = \mathbf X^T(\mathbf{X - XZ}) + \mu (\mathbf J - \mathbf Z\mathbf R + \frac{1}{\mu} \mathbf Y)\mathbf R^T$.

\item Set $\mathbf Z = \mathbf Z^k$, solve for $\mathbf J^{k+1}$ by
\begin{align*}
\lambda_2\|\mathbf J\|_{1,2} + \langle \mathbf Y_1, \mathbf J - \mathbf Z\mathbf R\rangle +\frac{\mu^k}2\|\mathbf J - \mathbf Z\mathbf R\|^2_F 
\end{align*}
which is equivalent to
\begin{align*}
\lambda_2\|\mathbf J\|_{1,2} + \frac{\mu^k}2\|\mathbf J - \mathbf Z\mathbf R + \frac{1}{\mu} \mathbf Y \|^2_F
\end{align*}
We linearise the last term which we call $F$
\begin{align*}
\min_{\mathbf J} \lambda_2\|\mathbf J\|_{1,2}+ \frac{\rho}{2} \| \mathbf J - (\mathbf J_{k-1} - \frac{1}{\rho} \partial F) \|_F^2
\end{align*}
and $\partial F = \mu (\mathbf J - \mathbf Z\mathbf R + \frac{1}{\mu} \mathbf Y)$.

\item Update $\mathbf Y$
\[
\mathbf Y = \mathbf Y + \mu (\mathbf J - \mathbf Z\mathbf R)
\]

\item Update $\mu$
\begin{align*}
 \mu^{k+1} = \textrm{min}( \mu_{\text{max}_1}, \gamma \mu^k)\\
\end{align*}
where $\gamma$ is defined as
\[
\gamma = 
\begin{cases}
\gamma^0 & \text{if} \;\; \mu^k \textrm{max} ( \| \mathbf Z^{k+1} - \mathbf Z^{k}  \|_F  , \|  \mathbf J^{k+1} - \mathbf J^{k} \|_F) < \epsilon_2 \\
1 & \text{otherwise,}
\end{cases}
\]

and $\mu^{\text{max}} \gg  \mu^0$.

\item Check stopping criteria
\[
\|\mathbf J^{k+1} - \mathbf Z^{k+1} \mathbf R \|_F < \epsilon_1, \;
\mu^k \sqrt{\rho} \; \textrm{max} ( \| \mathbf Z^{k+1} - \mathbf Z^{k}  \|_F  , \|  \mathbf J^{k+1} - \mathbf J^{k} \|) < \epsilon_2
\]

\end{enumerate}

\subsection{Exact Constraints}

Similar to the relaxed version we begin by introducing auxiliary variables
\begin{align}
\label{objective_exact}
\min_{\mathbf Z, \mathbf E, \mathbf J} \frac12\|\mathbf E\|^2_F +\lambda_1\|\mathbf Z\|_{1}+\lambda_2\| \mathbf J \|_{1,2}\\
\text{s.t.} \quad \mathbf{X = XZ + E}, \mathbf{J = ZR} \nonumber
\end{align}
We then form the Augmented Lagrangian to incorporate our constraints
\begin{align}
\mathcal{L}(\mathbf E, \mathbf Z, \mathbf J) = & \frac12\|\mathbf E\|^2_F + \lambda_1 \|\mathbf Z\|_{1} + \lambda_2\|\mathbf J\|_{1,2} \notag\\
& + \langle \mathbf Y_1, \mathbf{XZ} - \mathbf X + \mathbf E \rangle +\frac{\mu}2\|\mathbf{XZ} - \mathbf X + \mathbf E\|^2_F \notag\\
& + \langle \mathbf Y_2, \mathbf J - \mathbf Z\mathbf R\rangle +\frac{\mu}2\|\mathbf J - \mathbf Z\mathbf R\|^2_F
\end{align}
Then we solve for $\mathbf{E, Z, J}$ alternatively while fixing others in the following way

\begin{enumerate}

\item Set $\mathbf E = \mathbf E^k$ and $\mathbf J = \mathbf J^k$, solve for $\mathbf Z^{k+1}$ by
\begin{align*}
\min_{\mathbf Z} \lambda_1 \| \mathbf Z \|_1 + \langle \mathbf Y^k_1, \mathbf{XZ} - \mathbf X + \mathbf E \rangle +\frac{\mu^k}2\|\mathbf{XZ} - \mathbf X + \mathbf E\|^2_F\\
+ \langle \mathbf Y^k_2, \mathbf J - \mathbf Z\mathbf R\rangle +\frac{\mu^k}2\|\mathbf J - \mathbf Z\mathbf R\|^2_F
\end{align*}
\begin{align*}
\min_{\mathbf Z} \lambda_1 \| \mathbf Z \|_1 + \frac{\mu^k}2\|\mathbf{XZ} - (\mathbf X - \mathbf E - \frac{1}{\mu^k} \mathbf Y^k_1)\|^2_F \\ + \frac{\mu^k}2\| \mathbf Z\mathbf R - (\mathbf J - \frac{1}{\mu^k} \mathbf Y^k_2)\|^2_F
\end{align*}
We linearise the last two terms which we call $F$
\begin{align*}
\min_{\mathbf Z} \lambda_1 \| \mathbf Z \|_1 + \frac{\rho}{2} \| \mathbf Z - (\mathbf Z_{k-1} - \frac{1}{\rho} \partial F) \|_F^2
\end{align*}
and $\partial F = \mu^k \mathbf X^T (\mathbf{XZ_{k-1}} - (\mathbf X - \mathbf E - \frac{1}{\mu^k} \mathbf Y^k_1)) + \mu^k(\mathbf{Z_{k-1}R} - (\mathbf J - \frac{1}{\mu^k} \mathbf Y^k_2))\mathbf R^T$.

\item Set $\mathbf Z = \mathbf Z^k$ and $\mathbf J = \mathbf J^k$ solve for $\mathbf E^{k+1}$ by
\begin{align*}
\min_{\mathbf E} \frac12\|\mathbf E\|^2_F + \langle \mathbf Y^k_1, \mathbf{XZ} - \mathbf X + \mathbf E \rangle \\ + \frac{\mu^k}2\|\mathbf{XZ} - \mathbf X + \mathbf E\|^2_F 
\end{align*}
\begin{align*}
\min_{\mathbf E} \frac12\|\mathbf E\|^2_F + \frac{\mu^k}2\| \mathbf E - (\mathbf{XZ} - \mathbf X + \frac{1}{\mu^k} \mathbf Y^k_1) \|^2_F 
\end{align*}


\item Set $\mathbf Z = \mathbf Z^k$ and $\mathbf E = \mathbf E^k$ solve for $\mathbf J^{k+1}$ by
\begin{align*}
\min_{\mathbf J} \lambda_2\|\mathbf J\|_{1,2} + \langle \mathbf Y^k_2, \mathbf J - \mathbf Z\mathbf R\rangle +\frac{\mu^k}2\|\mathbf J - \mathbf Z\mathbf R\|^2_F
\end{align*}
\begin{align*}
\min_{\mathbf J} \lambda_2\|\mathbf J\|_{1,2} + \frac{\mu^k}2\|\mathbf J - (\mathbf Z\mathbf R - \frac{1}{\mu^k} \mathbf Y^k_2) \|^2_F
\end{align*}

\item Update $\mathbf Y_1$ and $\mathbf Y_2$
\begin{align*}
\mathbf Y^{k+1}_1 =& \mathbf Y^k_1 + \mu^k (\mathbf{XZ - X + E})\\
\mathbf Y^{k+1}_2 =& \mathbf Y^k_2 + \mu^k (\mathbf{J - ZR})
\end{align*}

\item Update $\mu$
\begin{align*}
 \mu^{k+1} = \textrm{min}( \mu_{\text{max}_1}, \gamma \mu^k)
\end{align*}
where $\gamma$ is defined as
\[
\gamma_1 = 
\begin{cases}
\gamma^0 & \text{if} \;\; \mu^k \sqrt{\rho} \frac{ \textrm{max} ( \| \mathbf Z^{k+1} - \mathbf Z^{k}  \|_F  , \|  \mathbf E^{k+1} - \mathbf E^{k} \|, \| \mathbf J^{k+1} - \mathbf J^{k}  \|_F  , \|  \mathbf Z^{k+1} \mathbf R - \mathbf Z^{k} \mathbf R \|_F)}{\| \mathbf X \|_F}  < \epsilon_2 \\
1 & \text{otherwise,}
\end{cases}
\]

where $\rho > \|\mathbf X\|_F^2$ and $\mu^{\text{max}} \gg  \mu^0$.

\item Check stopping criteria
\[
\frac{\|\mathbf{XZ^{k+1} - X + E^{k+1}} \|_F}{\| \mathbf X \|_F} < \epsilon_1, \frac{\|\mathbf{J^{k+1} - Z^{k+1}R}\|_F}{\| \mathbf X \|_F} < \epsilon_1
\]
\[
\mu^k \sqrt{\rho} \frac{ \textrm{max} ( \| \mathbf Z^{k+1} - \mathbf Z^{k}  \|_F  , \|  \mathbf E^{k+1} - \mathbf E^{k} \|, \| \mathbf J^{k+1} - \mathbf J^{k}  \|_F  , \|  \mathbf Z^{k+1} \mathbf R - \mathbf Z^{k} \mathbf R \|_F)}{\| \mathbf X \|_F}  < \epsilon_2
\]

\end{enumerate}



\newpage
\bibliographystyle{plain}
\bibliography{references}

\end{document}